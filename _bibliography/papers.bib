---
---
@inproceedings{RN711,
  bibtex_show={true},
  author = {Hu, Xuhui and Zeng, Hong and Chen, Dapeng and Zhu, Jiahang and Song, Aiguo},
  title = {Real-time continuous hand motion myoelectric decoding by automated data labeling},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {6951-6957},
  ISBN = {1728173957},
  year = {2020},
  month = {September},
  abstract={In this paper an automated data labeling (ADL) neural network is proposed to streamline dataset collecting for real-time predicting the continuous motion of hand and wrist, these gestures are only decoded from a surface electromyography (sEMG) array of eight channels. Unlike collecting both the bio-signals and hand motion signals as samples and labels in supervised learning, this algorithm only collects unlabeled sEMG into an unsupervised neural network, in which the hand motion labels are auto-generated. The coefficient of determination (R 2 ) for three DOFs, i.e. wrist flex/extension, wrist pro/supination, hand open/close, was 0.86, 0.89 and 0.87 respectively. The comparison between real motion labels and auto-generated labels shows that the latter has earlier response than former. The results of Fitts’ law test indicate that ADL has capability of controlling multi-DOFs simultaneously even though the training set only contains sEMG data from single DOF gesture. Moreover, no more hand motion measurement needed which greatly helps upper limb amputee imagine the gesture of residual limb to control a dexterous prosthesis.},
  html={https://ieeexplore.ieee.org/document/9197286},
  pdf={ICRA2020.pdf},
  publisher = {IEEE},
  dimensions={true},
  DOI = {10.1109/ICRA40945.2020.9197286},
  selected={true},
  preview={ICRA2020_preview.gif}
}

@article{RN712,
  bibtex_show={true},
  abbr={TNSRE},
  author = {Chen, Dapeng and Song, Aiguo and Hu, Xuhui and Fu, Liyue and Ouyang, Qiangqiang},
  title = {A spherical actuator-based hand-held haptic device for touch screen interaction},
  journal = {IEEE Access},
  volume = {7},
  pages = {15125-15139},
  ISSN = {2169-3536},
  year = {2019},
  publisher = {IEEE}
}

@article{RN713,
  bibtex_show={true},
  author = {Zeng, Hong and Shen, Yitao and Hu, Xuhui and Song, Aiguo and Xu, Baoguo and Li, Huijun and Wang, Yanxin and Wen, Pengcheng},
  title = {Semi-autonomous robotic arm reaching with hybrid gaze–brain machine interface},
  journal = {Frontiers in neurorobotics},
  volume = {13},
  pages = {111},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6992643/pdf/fnbot-13-00111.pdf},
  year = {2020},
  publisher = {Frontiers}
}

@article{RN714,
  bibtex_show={true},
  author = {Hu, Xuhui and Zeng, Hong and Song, Aiguo and Chen, Dapeng},
  title = {Robust continuous hand motion recognition using wearable array myoelectric sensor},
  journal = {IEEE Sensors Journal},
  volume = {21},
  number = {18},
  pages = {20596-20605},
  ISSN = {1530-437X},
  year = {2021},
  month = {July},
  abstract={With the advantages of comfortable wearing and outdoor usage, the myoelectric gesture recognition techniques have gained much attention in the field of human-machine interaction (HMI). The purpose of this study is to optimize model structure and transfer generalized features to improve the robustness of myoelectric hand motion decoding. We derived the hand motion recognition framework from the muscle synergy theory, which is formulated as a temporal convolutional (TC) model of array sEMG signals, then a hierarchical myoelectric decoding model was proposed to predict simultaneous and continuous hand motion. The model was trained by the methods of unsupervised low-level feature learning and automated data labeling to minimize training supervision. Extensive experiments on the public sEMG database (17 subjects in Biopatrec) show that the TC model can extract muscle synergy features with higher fidelity ( R 2 = 0.85±0.23) than the traditional instantaneous mixture model, the results of online test demonstrate robust myoelectric decoding on multiple simultaneous and continuous hand motions. More importantly, the analysis of weights visualization shows that the low-level feature representation layer of TC model can be migrated across the individuals, which provides a transferrable feature extraction layer for generalized hand motion decoding.},
  html={https://ieeexplore.ieee.org/abstract/document/9490243},
  pdf={JSEN2021.pdf},
  DOI = {10.1109/JSEN.2021.3098120},
  publisher = {IEEE},
  preview={JSEN2021_preview.gif}

}


@article{RN715,
  bibtex_show={true},
  author = {Chen, Dapeng and Liu, Jia and Tian, Lei and Hu, Xuhui and Song, Aiguo},
  title = {Research on the method of displaying the contour features of image to the visually impaired on the touch screen},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {29},
  pages = {2260-2270},
  ISSN = {1534-4320},
  year = {2021},
  publisher = {IEEE}
}

@article{RN716,
  bibtex_show={true},
  author = {Wei, Wentao and Hu, Xuhui and Liu, Hua and Zhou, Ming and Song, Yan},
  title = {Towards integration of domain knowledge-guided feature engineering and deep feature learning in surface electromyography-based hand movement recognition},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2021},
  year = {2021},
  publisher = {HINDAWI}
}


@article{RN717,
  bibtex_show={true},
  author = {Hu, Xuhui and Song, Aiguo and Zeng, Hong and Chen, Dapeng},
  title = {Intuitive environmental perception assistance for blind amputees using spatial audio rendering},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  volume = {4},
  number = {1},
  pages = {274-284},
  ISSN = {2576-3202},
  year = {2022},
  month = {January},
  publisher = {IEEE},
  abstract={Vision and touch are essential sensory systems for human to interact with the environment. For the blind amputees, how to quickly and intuitively convey the environmental information to them is one of the key issues for recovering their daily living ability. Inspired by the auditory localization ability of human, we constructed a virtual scene almost identical to reality, and concurrently added a virtual sound source to the interactive object. Leveraging the method of spatial audio rendering (SAR), the three-dimensional motion of the virtual sound source can be vividly simulated in real-time. Finally, a myoelectric prosthetic control system was developed to assist blind amputees in their daily activities, The Fitts’ law test on target localization was conducted on both SAR and voice prompt (VP) based path guidance methods, the results indicate that SAR significantly improves the information transfer rate. The results of prosthetic control test show that SAR reduces the completion time by half than the VP, while restoring the natural grasping path. With the advantage of intuitive and rich perception, the SAR demonstrated the potential applications for blind amputees to reconstruct the control and sensory loops.},
  html={https://ieeexplore.ieee.org/document/9693953},
  pdf={TMRB2022.pdf},
  dimensions={true},
  selected={true},
  DOI = {10.1109/TMRB.2022.3146743},
  preview={TMRB2022_preview.gif}
}

@article{RN719,
  bibtex_show={true},
  author = {Hu, Xuhui and Song, Aiguo and Wei, Zhikai and Zeng, Hong},
  title = {StereoPilot: A wearable target location system for blind and visually impaired using spatial audio rendering},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {30},
  pages = {1621-1630},
  ISSN = {1534-4320},
  year = {2022},
  month = {June},
  abstract={Vision loss severely impacts object recognition and spatial cognition for limited vision individuals. It is a challenge to compensate for this using other sensory modalities, such as touch or hearing. This paper introduces StereoPilot, a wearable target location system to facilitate the spatial cognition of BVI. Through wearing a head-mounted RGB-D camera, the 3D spatial information of the environment is measured and processed into navigation cues. Leveraging spatial audio rendering (SAR) technology, it allows the navigation cues to be transmitted in a type of 3D sound from which the sound orientation can be distinguished by the sound localization instincts in humans. Three haptic and auditory display strategies were compared with SAR through experiments with three BVI and four sighted subjects. Compared with mainstream speech instructional feedback, the experimental results of the Fitts’ law test showed that SAR increases the information transfer rate (ITR) by a factor of three for spatial navigation, while the positioning error is reduced by 40%. Furthermore, SAR has a lower learning effect than other sonification approaches such as vOICe. In desktop manipulation experiments, StereoPilot was able to obtain precise localization of desktop objects while reducing the completion time of target grasping tasks in half as compared to the voice instruction method. In summary, StereoPilot provides an innovative wearable target location solution that swiftly and intuitively transmits environmental information to BVI individuals in the real world.},
  html={https://ieeexplore.ieee.org/document/9795125},
  pdf={TNSRE2022.pdf},
  publisher = {IEEE},
  dimensions={true},
  selected={true},
  DOI = {10.1109/TNSRE.2022.3182661},
  preview={TNSRE2022_preview.jpg}
}

@article{RN720,
  bibtex_show={true},
  author = {Hu, Xuhui and Song, Aiguo and Wang, Jianzhi and Zeng, Hong and Wei, Wentao},
  title = {Finger movement recognition via high-density electromyography of intrinsic and extrinsic hand muscles},
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {373},
  DOI = {10.1038/s41597-022-01484-2},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/35768439},
  year = {2022},
  month = {June},
  abstract={Surface electromyography (sEMG) is commonly used to observe the motor neuronal activity within muscle fibers. However, decoding dexterous body movements from sEMG signals is still quite challenging. In this paper, we present a high-density sEMG (HD-sEMG) signal database that comprises simultaneously recorded sEMG signals of intrinsic and extrinsic hand muscles. Specifically, twenty able-bodied participants performed 12 finger movements under two paces and three arm postures. HD-sEMG signals were recorded with a 64-channel high-density grid placed on the back of hand and an 8-channel armband around the forearm. Also, a data-glove was used to record the finger joint angles. Synchronisation and reproducibility of the data collection from the HD-sEMG and glove sensors were ensured. The collected data samples were further employed for automated recognition of dexterous finger movements. The introduced dataset offers a new perspective to study the synergy between the intrinsic and extrinsic hand muscles during dynamic finger movements. As this dataset was collected from multiple participants, it also provides a resource for exploring generalized models for finger movement decoding.},
  html={https://www.nature.com/articles/s41597-022-01484-2},
  pdf={SDATA2022.pdf},
  publisher = {Nature},
  dimensions={true},
  selected={true},
  preview={SDATA2022_preview.png}
}

@inproceedings{RN721,
  abbr={AIM2022},
  bibtex_show={true},
  author = {Liu, Qi and Zhang, Jun and Li, Xinyi and Zhou, Jingsong and Hu, Xuhui and Jin, Weiming and Song, Aiguo},
  title = {A rigid and flexible structures coupled underactuated hand},
  booktitle = {2022 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
  publisher = {IEEE},
  pages = {1587-1592},
  year = {2022},
  ISBN = {1665413085}
}


@inproceedings{RN722,
  abbr={EMBC2022},
  bibtex_show={true},
  author = {Wei, Zhikai and Song, Aiguo and Hu, Xuhui},
  title = {Object localization assistive system based on CV and vibrotactile encoding},
  booktitle = {2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)},
  publisher = {IEEE},
  year = {2022},
  pages = {2882-2885},
  ISBN = {1728127823}
}

@article{RN723,
  abbr={ToH},
  bibtex_show={true},
  author = {Chen, Dapeng and Chen, Geng and Zhu, Dongliang and Hu, Xuhui and Wei, Zhong and Liu, Jia and Song, Aiguo},
  title = {Comparative experimental research on haptic display methods of virtual surface shape based on touch screen},
  journal = {IEEE Transactions on Haptics},
  volume = {15},
  number = {4},
  pages = {667-678},
  ISSN = {1939-1412},
  year = {2022},
  publisher = {IEEE}
}

@article{RN724,
  abbr={TNSRE},
  bibtex_show={true},
  author = {Zeng, Hong and Shen, Yitao and Sun, Dengfeng and Hu, Xuhui and Wen, Pengcheng and Liu, Jia and Song, Aiguo},
  title = {Extended control with hybrid gaze-bci for multi-robot system under hands-occupied dual-tasking},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  year = {2023},
  publisher={IEEE}
}

@article{RN725,
  abbr={TNSRE},
  bibtex_show={true},
  title = {Exploring biomimetic stiffness modulation and wearable finger haptics for improving myoelectric control of virtual hand},
  author = {Zeng, H. and Yu, W. and Chen, D. and Hu, X. and Zhang, D. and Song, A.},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {30},
  pages = {1601-1611},
  DOI = {10.1109/TNSRE.2022.3181284},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/35675253},
  year = {2022},
  publisher={IEEE}
}

@article{RN726,
  abbr={IEEE-RAL},
  bibtex_show={true},
  title = {Bridging Human-Robot Co-Adaptation via Biofeedback for Continuous Myoelectric Control},
  author = {Hu, X. and Song, A. and Zeng, H. and Wei, Z. and Deng, H. and Chen, D.},
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  pages = {8573-8580},
  DOI = {10.1109/LRA.2023.3330053},
  year = {2023},
  selected={true},
  publisher={IEEE}
}
